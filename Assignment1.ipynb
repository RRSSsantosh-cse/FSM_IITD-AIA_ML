{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvLKfuTMl0hcs+l4TY5tYI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RRSSsantosh-cse/FSM_IITD-AIA_ML/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnclnXC9iM45",
        "outputId": "58ddfef9-4aef-443f-d1d2-df94a7cbf28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Pre-processing refers to the transformations applied to our data before feeding it to the algorithm. Data Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\n",
        "\n",
        "\n",
        "\n",
        "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1) drops the specified columns ['PassengerId', 'Name', 'Ticket', 'Cabin'] from the DataFrame df. These columns are considered unnecessary for the analysis.\n",
        "\n",
        "df['Sex'] = df['Sex'].map({'female': 0, 'male': 1}).astype(int) maps the categorical values in the 'Sex' column to numerical values. 'female' is mapped to 0, and 'male' is mapped to 1. The map() function is used for the mapping, and astype(int) is used to convert the column to the integer data type.\n",
        "\n",
        "df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}) maps the categorical values in the 'Embarked' column to numerical values. 'S' is mapped to 0, 'C' is mapped to 1, and 'Q' is mapped to 2. Similar to the previous step, the map() function is used for the mapping.\n",
        "\n",
        "df['Age'].fillna(df['Age'].mean(), inplace=True) fills the missing values in the 'Age' column with the mean value of the column. The fillna() function is used to replace the missing values.\n",
        "\n",
        "df['Fare'].fillna(df['Fare'].mean(), inplace=True) fills the missing values in the 'Fare' column with the mean value of the column, using a similar approach as in the previous step.\n",
        "\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True) fills the missing values in the 'Embarked' column with the mode (most frequent value) of the column. The mode() function returns an array, so [0] is used to select the first element, which is the mode in this case.\n",
        "\n",
        "return df returns the preprocessed DataFrame df after applying all the above transformations.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cI-2dvqS8b-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocess_data(df):\n",
        "    # Drop unnecessary columns\n",
        "    df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "    \n",
        "    # Map categorical values to numerical\n",
        "    df['Sex'] = df['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
        "    df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "    \n",
        "    # Fill missing values\n",
        "    df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
        "    df['Fare'].fillna(df['Fare'].mean(), inplace=True)\n",
        "    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "a8hgP9OSiNqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization is the graphical representation of information and data in a pictorial or graphical format(Example: charts, graphs, and maps). Data visualization tools provide an accessible way to see and understand trends, patterns in data, and outliers. Data visualization tools and technologies are essential to analyzing massive amounts of information and making data-driven decisions. The concept of using pictures is to understand data that has been used for centuries. General types of data visualization are Charts, Tables, Graphs, Maps, Dashboards.\n",
        "\n",
        "survivors = df[df['Survived'] == 1] selects the rows from the DataFrame df where the 'Survived' column is equal to 1, indicating the survivors of the Titanic. This creates a new DataFrame named survivors.\n",
        "\n",
        "non_survivors = df[df['Survived'] == 0] selects the rows from the DataFrame df where the 'Survived' column is equal to 0, representing the non-survivors of the Titanic. This creates a new DataFrame named non_survivors.\n",
        "\n",
        "plt.bar(['Survived', 'Not Survived'], [len(survivors), len(non_survivors)]) creates a bar plot to show the count of survivors and non-survivors. The x-axis labels are 'Survived' and 'Not Survived', and the corresponding y-axis values are the lengths of the survivors and non_survivors DataFrames, respectively.\n",
        "\n",
        "plt.xlabel('Survival'), plt.ylabel('Count'), and plt.title('Number of Survivors') set the x-axis label, y-axis label, and title for the bar plot.\n",
        "\n",
        "plt.show() displays the bar plot.\n",
        "\n",
        "plt.scatter(survivors['Age'], survivors['Fare'], label='Survived', alpha=0.5) creates a scatter plot of the survivors. The x-axis represents the 'Age' of the passengers, and the y-axis represents the 'Fare' they paid. The points are labeled as 'Survived', and the transparency (alpha) is set to 0.5 to make the points semi-transparent.\n",
        "\n",
        "plt.scatter(non_survivors['Age'], non_survivors['Fare'], label='Not Survived', alpha=0.5) creates a scatter plot of the non-survivors using similar principles as in the previous step. The points are labeled as 'Not Survived'.\n",
        "\n",
        "plt.xlabel('Age'), plt.ylabel('Fare'), and plt.title('Age vs. Fare') set the x-axis label, y-axis label, and title for the scatter plot.\n",
        "\n",
        "plt.legend() displays a legend to distinguish between the 'Survived' and 'Not Survived' points.\n",
        "\n",
        "plt.show() displays the scatter plot.\n"
      ],
      "metadata": {
        "id": "DkEcwAxVhlqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(df):\n",
        "    # Count the number of survivors\n",
        "    survivors = df[df['Survived'] == 1]\n",
        "    non_survivors = df[df['Survived'] == 0]\n",
        "    \n",
        "    # Bar plot of the number of survivors\n",
        "    plt.bar(['Survived', 'Not Survived'], [len(survivors), len(non_survivors)])\n",
        "    plt.xlabel('Survival')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Number of Survivors')\n",
        "    plt.show()\n",
        "    \n",
        "    # Scatter plot of age vs. fare\n",
        "    plt.scatter(survivors['Age'], survivors['Fare'], label='Survived', alpha=0.5)\n",
        "    plt.scatter(non_survivors['Age'], non_survivors['Fare'], label='Not Survived', alpha=0.5)\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel('Fare')\n",
        "    plt.title('Age vs. Fare')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "t22IixIKgN5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function naive_bayes takes three parameters: x_train, y_train, and x_test. These parameters represent the training features, training labels, and test features, respectively.\n",
        "\n",
        "The code initializes an empty dictionary class_probs to store the probabilities of each class (label) in the training data. It will later be used to compute the prior probabilities of the classes.\n",
        "\n",
        "The for loop iterates over each unique label in y_train. Within the loop, it calculates the count of occurrences of each label in y_train and divides it by the total number of samples to obtain the class probabilities. These probabilities are stored in the class_probs dictionary.\n",
        "\n",
        "The code then initializes an empty dictionary feature_probs to store the probabilities of each feature value given a class. This dictionary will be used to compute the conditional probabilities of the features.\n",
        "\n",
        "The nested for loops iterate over each feature in x_train (represented by feature) and each unique label in y_train. Within these loops, the code calculates the count of occurrences of each feature value for a specific label. It then computes the probability of each feature value given the label by dividing the count by the total count of samples for that label. These probabilities are stored in the feature_probs dictionary.\n",
        "\n",
        "The code proceeds to predict the labels for the test data. It initializes an empty list predictions to store the predicted labels.\n",
        "\n",
        "The for loop iterates over each row (sample) in x_test. Within the loop, it initializes an empty dictionary label_probs to store the probabilities of each label for the current sample.\n",
        "\n",
        "The nested for loops iterate over each unique label in y_train. Within these loops, the code computes the label probability by multiplying the class probability (class_probs[label]) with the conditional probabilities of the feature values for the current sample. The conditional probabilities are retrieved from the feature_probs dictionary based on the feature index and feature value of the current sample.\n",
        "\n",
        "The code uses max(label_probs, key=label_probs.get) to determine the label with the highest probability and appends it to the predictions list.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXH0-cA9hoxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def naive_bayes(x_train, y_train, x_test):\n",
        "    # Compute class probabilities\n",
        "    class_probs = {}\n",
        "    for label in np.unique(y_train):\n",
        "        class_probs[label] = np.sum(y_train == label) / len(y_train)\n",
        "    \n",
        "    # Compute feature probabilities\n",
        "    feature_probs = {}\n",
        "    for feature in range(x_train.shape[1]):\n",
        "        feature_probs[feature] = {}\n",
        "        for label in np.unique(y_train):\n",
        "            feature_probs[feature][label] = {}\n",
        "            values, counts = np.unique(x_train[:, feature][y_train == label], return_counts=True)\n",
        "            total_count = np.sum(counts)\n",
        "            for value, count in zip(values, counts):\n",
        "                feature_probs[feature][label][value] = count / total_count\n",
        "    \n",
        "    # Predict the labels for the test data\n",
        "    predictions = []\n",
        "    for i in range(x_test.shape[0]):\n",
        "        label_probs = {}\n",
        "        for label in np.unique(y_train):\n",
        "            label_probs[label] = class_probs[label]\n",
        "            for feature in range(x_test.shape[1]):\n",
        "                value = x_test[i, feature]\n",
        "                if value in feature_probs[feature][label]:\n",
        "                    label_probs[label] *= feature_probs[feature][label][value]\n",
        "        \n",
        "        predictions.append(max(label_probs, key=label_probs.get))\n",
        "    \n",
        "    return np.array(predictions)"
      ],
      "metadata": {
        "id": "1giTbXwXgPON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function k_nearest_neighbors takes four parameters: x_train, y_train, x_test, and k. These parameters represent the training features, training labels, test features, and the number of nearest neighbors to consider, respectively.\n",
        "\n",
        "The code computes the Euclidean distances between each pair of training and test samples using the formula np.sqrt(np.sum((x_train[:, np.newaxis] - x_test)**2, axis=2)). It subtracts each test sample from all training samples, squares the differences element-wise, sums them along the axis representing the features, and finally takes the square root. This results in a matrix of distances where each row represents a test sample and each column represents the distance to a training sample.\n",
        "\n",
        "The code then uses np.argsort(distances, axis=0)[:k] to get the indices of the k nearest neighbors for each test sample. It sorts the distances along the rows (test samples) and returns the indices of the k smallest distances. This results in a matrix of indices where each row represents the indices of the k nearest neighbors for a test sample.\n",
        "\n",
        "Next, the code retrieves the labels of the k nearest neighbors using the obtained indices. It assigns y_train[knn_indices] to the variable knn_labels. This matrix has the same shape as the knn_indices matrix, where each row represents the labels of the k nearest neighbors for a test sample.\n",
        "\n",
        "The code proceeds to predict the labels for the test data based on majority voting. It uses np.apply_along_axis to apply a lambda function to each row (test sample) of the knn_labels matrix. The lambda function uses np.bincount to count the occurrences of each label and returns the label with the highest count using np.argmax. This results in an array of predicted labels.\n",
        "\n",
        "Finally, the function returns the array of predicted labels (predictions)."
      ],
      "metadata": {
        "id": "wF407x-_iKyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def k_nearest_neighbors(x_train, y_train, x_test, k):\n",
        "    # Compute the Euclidean distances between each pair of training and test samples\n",
        "    distances = np.sqrt(np.sum((x_train[:, np.newaxis] - x_test)**2, axis=2))\n",
        "    \n",
        "    # Get the indices of the k nearest neighbors for each test sample\n",
        "    knn_indices = np.argsort(distances, axis=0)[:k]\n",
        "    \n",
        "    # Get the labels of the k nearest neighbors\n",
        "    knn_labels = y_train[knn_indices]\n",
        "    \n",
        "    # Predict the labels for the test data based on majority voting\n",
        "    predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=knn_labels)\n",
        "    \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "G_2oEnUegPZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code reads the training data from the CSV file located at '/content/drive/MyDrive/Machine_Learning/train.csv' using pd.read_csv. It assigns the DataFrame to train_df.\n",
        "\n",
        "Similarly, the code reads the test data from the CSV file located at '/content/drive/MyDrive/Machine_Learning/test.csv' using pd.read_csv. It assigns the DataFrame to test_df.\n",
        "\n",
        "It also reads the ground truth data from the CSV file located at '/content/drive/MyDrive/Machine_Learning/gender_submission.csv' using pd.read_csv. It assigns the DataFrame to gender_submission_df.\n",
        "\n",
        "The code preprocesses the training and test data by applying the preprocess_data function to train_df and test_df respectively. This function performs data cleaning, mapping categorical values to numerical values, and filling missing values. The preprocessed training data is assigned back to train_df, and the preprocessed test data is assigned back to test_df.\n",
        "The code visualizes the preprocessed training data using the visualize_data function, which is not shown in the provided code snippet.\n",
        "\n",
        "The training data is split into features (x_train) and labels (y_train). The features are obtained by dropping the 'Survived' column from train_df using .drop('Survived', axis=1).values, and the labels are obtained by selecting the 'Survived' column from train_df using ['Survived'].values.\n",
        "\n",
        "The test data is split into features (x_test) by dropping the 'Pclass' column from test_df using .drop('Pclass', axis=1).values.\n",
        "The code applies the Naive Bayes algorithm by calling the naive_bayes function with x_train, y_train, and x_test as inputs. It assigns the predictions to naive_bayes_predictions.\n",
        "\n",
        "Similarly, the code applies the K-Nearest Neighbors algorithm by calling the k_nearest_neighbors function with x_train, y_train, x_test, and k (number of neighbors) as inputs. It assigns the predictions to knn_predictions.\n",
        "The ground truth labels are extracted from gender_submission_df and assigned to ground_truth.\n",
        "\n",
        "The code calculates the accuracies of the Naive Bayes and K-Nearest Neighbors predictions by comparing them with the ground truth labels. It uses np.mean to calculate the fraction of correct predictions and multiplies it by 100 to get the percentage accuracy.\n",
        "Finally, the code prints the accuracies of Naive Bayes and K-Nearest Neighbors predictions."
      ],
      "metadata": {
        "id": "YX4hng8xiXVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Titanic dataset using Pandas\n",
        "p='/content/drive/MyDrive/Machine_Learning/train.csv'\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(p)\n",
        "pa='/content/drive/MyDrive/Machine_Learning/test.csv'\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(pa)\n",
        "\n",
        "path='/content/drive/MyDrive/Machine_Learning/gender_submission.csv'\n",
        "\n",
        "gender_submission_df = pd.read_csv(path)\n",
        "\n",
        "# Preprocess the data\n",
        "train_df = preprocess_data(train_df)\n",
        "test_df = preprocess_data(test_df)\n",
        "\n",
        "# Visualize the data\n",
        "visualize_data(train_df)\n",
        "\n",
        "# Split the training data into features and labels\n",
        "x_train = train_df.drop('Survived', axis=1).values\n",
        "y_train = train_df['Survived'].values\n",
        "\n",
        "# Split the test data into features and labels\n",
        "x_test = test_df.drop('Pclass', axis=1).values\n",
        "\n",
        "# Apply Naive Bayes\n",
        "naive_bayes_predictions = naive_bayes(x_train, y_train, x_test)\n",
        "\n",
        "# Apply K-Nearest Neighbors\n",
        "k = 5  # Number of neighbors\n",
        "knn_predictions = k_nearest_neighbors(x_train, y_train, x_test, k)\n",
        "\n",
        "# Compare the predictions with the ground truth\n",
        "ground_truth = gender_submission_df['Survived'].values\n",
        "\n",
        "# Calculate accuracy\n",
        "naive_bayes_accuracy = np.mean(naive_bayes_predictions == ground_truth) * 100\n",
        "knn_accuracy = np.mean(knn_predictions == ground_truth) * 100\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Naive Bayes Accuracy: {:.2f}%\".format(naive_bayes_accuracy))\n",
        "print(\"K-Nearest Neighbors Accuracy: {:.2f}%\".format(knn_accuracy))\n"
      ],
      "metadata": {
        "id": "U9VNga-MgPj6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}